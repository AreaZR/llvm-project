; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc < %s -mtriple=arm64-apple-darwin | FileCheck %s

define i8 @test_64bit_add(ptr %a, i64 %b) {
  %tmp1 = getelementptr inbounds i16, ptr %a, i64 %b
  %tmp2 = load i16, ptr %tmp1
  %tmp3 = trunc i16 %tmp2 to i8
  ret i8 %tmp3
}

; These tests are trying to form SEXT and ZEXT operations that never leave i64
; space, to make sure LLVM can adapt the offset register correctly.
define void @ldst_8bit(ptr %base, i64 %offset) minsize {
   %off32.sext.tmp = shl i64 %offset, 32
   %off32.sext = ashr i64 %off32.sext.tmp, 32
   %addr8_sxtw = getelementptr i8, ptr %base, i64 %off32.sext
   %val8_sxtw = load volatile i8, ptr %addr8_sxtw
   %val32_signed = sext i8 %val8_sxtw to i32
   store volatile i32 %val32_signed, ptr @var_32bit

  %addrint_uxtw = ptrtoint ptr %base to i64
  %offset_uxtw = and i64 %offset, 4294967295
  %addrint1_uxtw = add i64 %addrint_uxtw, %offset_uxtw
  %addr_uxtw = inttoptr i64 %addrint1_uxtw to ptr
  %val8_uxtw = load volatile i8, ptr %addr_uxtw
  %newval8 = add i8 %val8_uxtw, 1
  store volatile i8 %newval8, ptr @var_8bit

   ret void
}


define void @ldst_16bit(ptr %base, i64 %offset) minsize {
  %addrint_uxtw = ptrtoint ptr %base to i64
  %offset_uxtw = and i64 %offset, 4294967295
  %addrint1_uxtw = add i64 %addrint_uxtw, %offset_uxtw
  %addr_uxtw = inttoptr i64 %addrint1_uxtw to ptr
  %val8_uxtw = load volatile i16, ptr %addr_uxtw
  %newval8 = add i16 %val8_uxtw, 1
  store volatile i16 %newval8, ptr @var_16bit

  %base_sxtw = ptrtoint ptr %base to i64
  %offset_sxtw.tmp = shl i64 %offset, 32
  %offset_sxtw = ashr i64 %offset_sxtw.tmp, 32
  %addrint_sxtw = add i64 %base_sxtw, %offset_sxtw
  %addr_sxtw = inttoptr i64 %addrint_sxtw to ptr
  %val16_sxtw = load volatile i16, ptr %addr_sxtw
  %val64_signed = sext i16 %val16_sxtw to i64
  store volatile i64 %val64_signed, ptr @var_64bit


  %base_uxtwN = ptrtoint ptr %base to i64
  %offset_uxtwN = and i64 %offset, 4294967295
  %offset2_uxtwN = shl i64 %offset_uxtwN, 1
  %addrint_uxtwN = add i64 %base_uxtwN, %offset2_uxtwN
  %addr_uxtwN = inttoptr i64 %addrint_uxtwN to ptr
  %val32 = load volatile i32, ptr @var_32bit
  %val16_trunc32 = trunc i32 %val32 to i16
  store volatile i16 %val16_trunc32, ptr %addr_uxtwN
   ret void
}

define void @ldst_32bit(ptr %base, i64 %offset) minsize {
  %addrint_uxtw = ptrtoint ptr %base to i64
  %offset_uxtw = and i64 %offset, 4294967295
  %addrint1_uxtw = add i64 %addrint_uxtw, %offset_uxtw
  %addr_uxtw = inttoptr i64 %addrint1_uxtw to ptr
  %val32_uxtw = load volatile i32, ptr %addr_uxtw
  %newval32 = add i32 %val32_uxtw, 1
  store volatile i32 %newval32, ptr @var_32bit

  %base_sxtw = ptrtoint ptr %base to i64
  %offset_sxtw.tmp = shl i64 %offset, 32
  %offset_sxtw = ashr i64 %offset_sxtw.tmp, 32
  %addrint_sxtw = add i64 %base_sxtw, %offset_sxtw
  %addr_sxtw = inttoptr i64 %addrint_sxtw to ptr
  %val32_sxtw = load volatile i32, ptr %addr_sxtw
  %val64_signed = sext i32 %val32_sxtw to i64
  store volatile i64 %val64_signed, ptr @var_64bit


  %base_uxtwN = ptrtoint ptr %base to i64
  %offset_uxtwN = and i64 %offset, 4294967295
  %offset2_uxtwN = shl i64 %offset_uxtwN, 2
  %addrint_uxtwN = add i64 %base_uxtwN, %offset2_uxtwN
  %addr_uxtwN = inttoptr i64 %addrint_uxtwN to ptr
  %val32 = load volatile i32, ptr @var_32bit
  store volatile i32 %val32, ptr %addr_uxtwN
   ret void
}

define void @ldst_64bit(ptr %base, i64 %offset) minsize {
  %addrint_uxtw = ptrtoint ptr %base to i64
  %offset_uxtw = and i64 %offset, 4294967295
  %addrint1_uxtw = add i64 %addrint_uxtw, %offset_uxtw
  %addr_uxtw = inttoptr i64 %addrint1_uxtw to ptr
  %val64_uxtw = load volatile i64, ptr %addr_uxtw
  %newval8 = add i64 %val64_uxtw, 1
  store volatile i64 %newval8, ptr @var_64bit

  %base_sxtw = ptrtoint ptr %base to i64
  %offset_sxtw.tmp = shl i64 %offset, 32
  %offset_sxtw = ashr i64 %offset_sxtw.tmp, 32
  %addrint_sxtw = add i64 %base_sxtw, %offset_sxtw
  %addr_sxtw = inttoptr i64 %addrint_sxtw to ptr
  %val64_sxtw = load volatile i64, ptr %addr_sxtw
  store volatile i64 %val64_sxtw, ptr @var_64bit


  %base_uxtwN = ptrtoint ptr %base to i64
  %offset_uxtwN = and i64 %offset, 4294967295
  %offset2_uxtwN = shl i64 %offset_uxtwN, 3
  %addrint_uxtwN = add i64 %base_uxtwN, %offset2_uxtwN
  %addr_uxtwN = inttoptr i64 %addrint_uxtwN to ptr
  %val64 = load volatile i64, ptr @var_64bit
  store volatile i64 %val64, ptr %addr_uxtwN
   ret void
}

@var_8bit = global i8 0
@var_16bit = global i16 0
@var_32bit = global i32 0
@var_64bit = global i64 0
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
